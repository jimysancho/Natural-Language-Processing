{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vocabulary import Vocabulary\n",
    "from utils.word2vec import Word2Vec\n",
    "from RNN import RNN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772053c",
   "metadata": {},
   "source": [
    "# 1. Train word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80517c32",
   "metadata": {},
   "source": [
    "Before trying to predict the next word given a sequence, we need to obtain the word vector space using the word2vec algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43710db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/timemachine.txt'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    file = f.read().strip().lower().split('\\n')\n",
    "\n",
    "words = []\n",
    "for line in file:\n",
    "    words += line.split(' ')\n",
    "    \n",
    "w2v = Word2Vec(Vocabulary(path), dim=300, window_size=2, \n",
    "               lr=0.02, random_state=10, K=2, \n",
    "               distribute=True)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "max_words = 10_000\n",
    "words = words[:max_words]\n",
    "\n",
    "for j in range(50):\n",
    "    total_loss = 0\n",
    "    for i in range(w2v.window_size, len(words) - w2v.window_size):\n",
    "        window = [words[i+j] for j in range(-w2v.window_size, w2v.window_size + 1) if j != 0]\n",
    "        center_word = words[i]\n",
    "        loss, n_s_i = w2v.forward(window, center_word)\n",
    "        grads = w2v.backward(window, center_word, n_s_i)            \n",
    "        w2v.update(grads)\n",
    "        total_loss += loss \n",
    "    \n",
    "    if not ((j + 1) % 10): print(f'Cost epoch {j+1}th: ', np.round(total_loss, decimals=3))\n",
    "    cost.append(total_loss)\n",
    "\n",
    "plt.plot(cost, '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77240b65",
   "metadata": {},
   "source": [
    "# 2. Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16094dbb",
   "metadata": {},
   "source": [
    "1. Data preprocessing: input seq ---> target seq where the target seq is the input seq minus the first word, and the last word is the next word of the input seq. \n",
    "\n",
    "2. We want to apply a Many-to-One architecture, where the target word is the next word given a sequence. In order to do that, the Output layer must have a size of V, where V is the length of the vocabulary. \n",
    "\n",
    "3. To train we apply the softmax over the hole vocabulary using the softmax function as the output activation function, trying to maximize the probability of the target word. \n",
    "\n",
    "4. To predict, we apply a forward pass of a given sequence. The output will be the word whose probability is maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062092e8",
   "metadata": {},
   "source": [
    "## 2.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082a153",
   "metadata": {},
   "source": [
    "We create some classes to make the preprocess and the training of the model more simple and easily tunned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset: \n",
    "    \n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert len(self.inputs) == len(self.outputs)\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, (list, int))\n",
    "        if isinstance(index, int):\n",
    "            return self.inputs[index], self.outputs[index]\n",
    "        return [self.inputs[i] for i in index], [self.outputs[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model: \n",
    "    \n",
    "    def __init__(self, word_text, word2vec, length=5):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - word_text: list. \n",
    "                     List of the text we will use to train our model. \n",
    "        - word2vec: Word2Vec\n",
    "                    Trained word2vec\n",
    "        - length: int\n",
    "                  How many past words we will use to predict the next one\n",
    "        \"\"\"\n",
    "        self.words = word_text\n",
    "        self.word2vec = word2vec\n",
    "        self.sequence_length = length\n",
    "        self.rnn = None\n",
    "                \n",
    "    def create_dataset(self):\n",
    "        \n",
    "        input_words, target_words = [], []\n",
    "        input_vectors, output_vectors = [], []\n",
    "        \n",
    "        def one_hot(index):\n",
    "            o_h = np.zeros((len(self.word2vec.vocabulary), 1))\n",
    "            o_h[index] = 1\n",
    "            return o_h\n",
    "        \n",
    "        for i in range(self.sequence_length, len(self.words) - self.sequence_length):\n",
    "            input_seq = self.words[i: i + self.sequence_length]\n",
    "            target_word = self.words[i + self.sequence_length]\n",
    "            \n",
    "            input_words.append(input_seq)\n",
    "            target_words.append(target_word)\n",
    "            \n",
    "            input_vector_indexes = [self.word2vec.vocabulary[word] for word in input_seq]\n",
    "            target_vector_indexes = self.word2vec.vocabulary[target_word]\n",
    "                            \n",
    "            input_vector = [self.word2vec.U[:, index] for index in input_vector_indexes]\n",
    "            target_vector = one_hot(target_vector_indexes)\n",
    "            \n",
    "            input_vectors.append(input_vector)\n",
    "            output_vectors.append(target_vector)\n",
    "            \n",
    "        self.word_dataset = Dataset(np.array(input_words), np.array(target_words))\n",
    "        self.vector_dataset = Dataset(np.array(input_vectors), np.array(output_vectors))\n",
    "            \n",
    "        return self.word_dataset, self.vector_dataset\n",
    "    \n",
    "    def train_model(self, model, epochs=10):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - model: Recurrent Neural Network\n",
    "        - epochs: int\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = self.vector_dataset\n",
    "        \n",
    "        cost = []\n",
    "        print_ratio = epochs // 10\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for input_t, target_t in dataset:\n",
    "                loss_t, predictions, outputs, hidden_outputs, hidden_states = model.forward(input_t, target_t)\n",
    "\n",
    "                model.backward(input_t, target_t, predictions, outputs, hidden_outputs, hidden_states)\n",
    "                model.update_weigths()\n",
    "                \n",
    "                loss += loss_t\n",
    "\n",
    "            cost.append(loss)\n",
    "\n",
    "            if not (epoch + 1) % print_ratio:\n",
    "                print(f'Loss epoch {epoch + 1}th: ', np.round(loss, decimals=3))\n",
    "                \n",
    "        self.rnn = model\n",
    "                \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, distribution=False):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - distribution: bool\n",
    "                        If True, the next word will be drawn from the softmax distribution\n",
    "                        If False, the next word will be the maximum argument of the softmax\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for input_seq, _ in self.vector_dataset:\n",
    "            prediction = self.rnn.predict(input_seq)\n",
    "            if distribution:\n",
    "                max_prob_index = np.random.choice(len(prediction), p=prediction.ravel())\n",
    "                predictions.append(max_prob_index)\n",
    "            else:\n",
    "                predictions.append(np.argmax(prediction))\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68760faa",
   "metadata": {},
   "source": [
    "## 2.2 Creation of the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(word_text=words[1000: 1400].copy(), word2vec=w2v, length=2)\n",
    "words_dataset, vectors_dataset = model.create_dataset()\n",
    "print('Total number of input and target sequences: ', len(words_dataset))\n",
    "print(\" \")\n",
    "\n",
    "example = words_dataset[5]\n",
    "input_seq_example = ''\n",
    "for word in example[0]:\n",
    "    input_seq_example += word + ' '\n",
    "\n",
    "print('Example: ')\n",
    "print('Input sequence: ', input_seq_example)\n",
    "print('Target words: ', example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80224c",
   "metadata": {},
   "source": [
    "## 2.3 RNN parameters and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeaa0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_units = w2v.dim\n",
    "output_units = len(w2v.vocabulary)\n",
    "hidden_units = 80\n",
    "learning_rate = 0.005\n",
    "\n",
    "rnn = RNN(input_units=input_units, output_units=output_units, \n",
    "           lr=learning_rate, hidden_units=hidden_units, \n",
    "           dtype='many-to-one', cost_type='negative-log')\n",
    "\n",
    "cost = model.train_model(rnn, epochs=100)\n",
    "plt.plot(cost, '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38c16b",
   "metadata": {},
   "source": [
    "## 2.4 Prediction on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ef564",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(distribution=False)\n",
    "correct_predictions = 0\n",
    "n_examples = 10\n",
    "\n",
    "for n, ((input_words, target_word), prediction) in enumerate(zip(words_dataset, predictions)):\n",
    "    input_seq = ''\n",
    "    for word in input_words:\n",
    "        input_seq += word + ' '\n",
    "    prediction_word = w2v.vocabulary[prediction]\n",
    "        \n",
    "    if target_word == prediction_word:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    if n < n_examples:\n",
    "        print('Input sequence: ', input_seq)\n",
    "        print('Target word: ', target_word)\n",
    "        print('Predicted word: ', prediction_word)\n",
    "        print(\" \")\n",
    "        \n",
    "print('Accuracy: ', str(np.round(correct_predictions / len(words_dataset), decimals=3) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b4784",
   "metadata": {},
   "source": [
    "# 3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_words=5):\n",
    "    input_vector_indexes = [model.word2vec.vocabulary[word] for word in input_sequence]\n",
    "    input_vector = [model.word2vec.U[:, index] for index in input_vector_indexes]\n",
    "    prediction = model.rnn.predict(input_vector)\n",
    "    indexes = np.argsort(prediction.ravel())[::-1][:max_words]\n",
    "    return indexes\n",
    "\n",
    "input_sequences = [['dimensions'], ['put', 'in'], ['they'], ['he', 'travel', 'with'], \n",
    "                   ['the', 'time', 'traveller'], ['made', 'of']]\n",
    "for input_sequence in input_sequences:\n",
    "    prediction = predict(model, input_sequence)\n",
    "    phrase = ''\n",
    "    for word in input_sequence:\n",
    "        phrase += word + ' '\n",
    "    print('Input sequence: ', phrase)\n",
    "    print('Most probable words: ')\n",
    "    for i, p in enumerate(prediction):\n",
    "        print(f'{i+1}: ', model.word2vec.vocabulary[p])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfb2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
